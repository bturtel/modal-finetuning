{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Deepseek R1\n",
    "Here we're trying to implement [Mini Deepseek R1](https://www.philschmid.de/mini-deepseek-r1) using Modal.\n",
    "\n",
    "\n",
    "References:\n",
    "- [Mini Deepseek R1](https://www.philschmid.de/mini-deepseek-r1)\n",
    "- [How to Launch a Jupyter Notebook on Modal Programmatically](https://modal.com/blog/how_to_launch_a_jupyter_notebook_on_modal_programmatically_article)\n",
    "- [Modal Examples: Basic Notebook](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/basic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Jupyter notebook local python MUST MATCH the python version in the Modal image.\n",
    "\n",
    "Create a new venv for 3.11:\n",
    "```\n",
    "python3.11 -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "Copied the Modal image [here](https://github.com/modal-labs/modal-client/blob/d924202767ef313ba67d60451975432e09e7b760/modal/image.py#L962C17-L962C55)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets huggingface_hub jinja2 modal\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from the Hugging Face Hub...\n",
      "Dataset loaded. Sample count: 50000\n",
      "Data preprocessing complete. Train size: 45000 Test size: 5000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset from the Hugging Face Hub...\")\n",
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "# Select a random subset of 50k samples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))\n",
    "print(\"Dataset loaded. Sample count:\", len(dataset))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "def generate_r1_prompt(example):\n",
    "    r1_prefix = [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in your mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {example['nums']}, create an equation that equals {example['target']}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True)\n",
    "    return {\"prompt\": prompt, \"target\": example[\"target\"], \"nums\": example[\"nums\"]}\n",
    "\n",
    "dataset = dataset.map(generate_r1_prompt)\n",
    "\n",
    "# Split dataset into train and test sets.\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Data preprocessing complete. Train size:\", len(train_dataset), \"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_dataset.json\", \"w\") as f:\n",
    "    json.dump(train_dataset.to_dict(), f)\n",
    "with open(\"data/test_dataset.json\", \"w\") as f:\n",
    "    json.dump(train_dataset.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_reward_func(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Checks for proper format: <think>...</think>\\n<answer>...</answer>\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt in zip(completions, target):\n",
    "        try:\n",
    "            # Add synthetic <think> tag as it prefaces the assistant's output.\n",
    "            completion = \"<think>\" + completion  \n",
    "            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "            match = re.search(regex, completion, re.DOTALL)\n",
    "            if match is None or len(match.groups()) != 2:\n",
    "                rewards.append(0.0)\n",
    "            else:\n",
    "                rewards.append(1.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def equation_reward_func(completions, target, nums, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates correctness of the equation in the output.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt, numbers in zip(completions, target, nums):\n",
    "        try:\n",
    "            completion = \"<think>\" + completion\n",
    "            match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "            if match is None:\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            equation = match.group(1).strip()\n",
    "            # Extract and compare used numbers.\n",
    "            used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
    "            if sorted(used_numbers) != sorted(numbers):\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "            if not re.match(allowed_pattern, equation):\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            result = eval(equation, {\"__builtins__\": None}, {})\n",
    "            if abs(float(result) - float(gt)) < 1e-5:\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Run quick tests on the reward functions here if desired.\n",
    "correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n",
    "exactly once, with basic arithmetic operations, that equals 65. One possible\n",
    "combination is 55 + 36 - 19 + 7... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "correct_sample_2 = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
    " \n",
    "wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:                      \n",
    "95 + 88 = 183                                                                                                              \n",
    "Now, let's subtract 104 from 183 to get 79:\n",
    "183 - 104 = 79\n",
    "<think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n",
    " \n",
    "wrong_result = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 18 </answer>\"\"\"\n",
    " \n",
    " \n",
    "test_rewards = format_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 1.0], \"Reward function is not working\"\n",
    "test_rewards = equation_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Modal\n",
    "- [Modal](https://modal.com/): [Docs](https://modal.com/docs)  |  [Logs](https://modal.com/logs)  |  [Github](https://github.com/modal-labs/modal-client)  |  [Notebook Example](https://modal.com/docs/guide/notebooks)\n",
    "- Follow [this guide](https://modal.com/docs/guide) to create and connect an account on Modal\n",
    "- In [Modal Secrets](https://modal.com/secrets), add a HuggingFace secret (HF_SECRET) with your HuggingFace token (HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m app\u001b[38;5;241m.\u001b[39mrun():\n\u001b[0;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/synchronicity/synchronizer.py:592\u001b[0m, in \u001b[0;36mSynchronizer._wrap_proxy_method.<locals>.proxy_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[synchronizer_self\u001b[38;5;241m.\u001b[39m_original_attr]\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserCodeException \u001b[38;5;28;01mas\u001b[39;00m uc_exc:\n\u001b[1;32m    594\u001b[0m     uc_exc\u001b[38;5;241m.\u001b[39mexc\u001b[38;5;241m.\u001b[39m__suppress_context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/synchronicity/combined_types.py:29\u001b[0m, in \u001b[0;36mFunctionWithAio.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserCodeException \u001b[38;5;28;01mas\u001b[39;00m uc_exc:\n\u001b[1;32m     28\u001b[0m     uc_exc\u001b[38;5;241m.\u001b[39mexc\u001b[38;5;241m.\u001b[39m__suppress_context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m uc_exc\u001b[38;5;241m.\u001b[39mexc\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_object.py:284\u001b[0m, in \u001b[0;36mlive_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydrate()\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_functions.py:1345\u001b[0m, in \u001b[0;36m_Function.remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_generator:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\n\u001b[1;32m   1342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA generator function cannot be called with `.remote(...)`. Use `.remote_gen(...)` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1343\u001b[0m     )\n\u001b[0;32m-> 1345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_function(args, kwargs)\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_functions.py:1295\u001b[0m, in \u001b[0;36m_Function._call_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     function_call_invocation_type \u001b[38;5;241m=\u001b[39m api_pb2\u001b[38;5;241m.\u001b[39mFUNCTION_CALL_INVOCATION_TYPE_SYNC_LEGACY\n\u001b[1;32m   1287\u001b[0m invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _Invocation\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1289\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     function_call_invocation_type\u001b[38;5;241m=\u001b[39mfunction_call_invocation_type,\n\u001b[1;32m   1293\u001b[0m )\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m invocation\u001b[38;5;241m.\u001b[39mrun_function()\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_functions.py:245\u001b[0m, in \u001b[0;36m_Invocation.run_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_context\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m ctx\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mretry_policy\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mretries \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mfunction_call_invocation_type \u001b[38;5;241m!=\u001b[39m api_pb2\u001b[38;5;241m.\u001b[39mFUNCTION_CALL_INVOCATION_TYPE_SYNC\n\u001b[1;32m    244\u001b[0m ):\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_output()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# User errors including timeouts are managed by the user specified retry policy.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m user_retry_manager \u001b[38;5;241m=\u001b[39m RetryManager(ctx\u001b[38;5;241m.\u001b[39mretry_policy)\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_functions.py:234\u001b[0m, in \u001b[0;36m_Invocation._get_single_output\u001b[0;34m(self, expected_jwt)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_single_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, expected_jwt: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# waits indefinitely for a single result for the function, and clear the outputs buffer after\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     item: api_pb2\u001b[38;5;241m.\u001b[39mFunctionGetOutputsItem \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop_function_call_outputs(\n\u001b[1;32m    229\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    233\u001b[0m     )\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _process_result(item\u001b[38;5;241m.\u001b[39mresult, item\u001b[38;5;241m.\u001b[39mdata_format, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient)\n",
      "File \u001b[0;32m~/dev/modal/venv/lib/python3.11/site-packages/modal/_utils/function_utils.py:505\u001b[0m, in \u001b[0;36m_process_result\u001b[0;34m(result, data_format, stub, client)\u001b[0m\n\u001b[1;32m    503\u001b[0m         uc_exc \u001b[38;5;241m=\u001b[39m UserCodeException(exc_with_hints(exc))\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m uc_exc\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteError(result\u001b[38;5;241m.\u001b[39mexception)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_data_format(data, data_format, client)\n",
      "\u001b[0;31mRemoteError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import modal\n",
    "\n",
    "app = modal.App()\n",
    "\n",
    "image = (  \n",
    "    modal.Image.from_registry(\"nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04\", add_python=\"3.11\")\n",
    "    .apt_install(\"git\", \"cmake\", \"clang\", \"build-essential\", \"libgomp1\")\n",
    "    .run_commands(\n",
    "        \"git config --global credential.helper store\",\n",
    "        \"git config --global --add safe.directory '*'\"\n",
    "    )\n",
    "    # .entrypoint([])\n",
    "    .pip_install(\n",
    "        \"torch==2.5.1\", \n",
    "        \"tensorboard\", \n",
    "        \"setuptools<71.0.0\",\n",
    "        \"huggingface_hub\",\n",
    "        \"trl==0.14.0\",\n",
    "        \"bitsandbytes\",\n",
    "        \"ninja\",\n",
    "        \"peft\",\n",
    "        \"transformers==4.48.1\",\n",
    "        \"accelerate==1.3.0\",\n",
    "        \"hf-transfer==0.1.9\",\n",
    "        \"deepspeed==0.15.4\",\n",
    "        \"vllm==0.7.0\",\n",
    "        \"datasets\",\n",
    "    )\n",
    "    .pip_install(\"flash-attn==2.5.8\") #, extra_options=\"--no-build-isolation\")\n",
    "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
    "    .add_local_dir(\"data\", remote_path=\"/mnt/data\")\n",
    ")\n",
    "\n",
    "@app.function(image=image, \n",
    "              gpu=\"A100\", # For more:  gpu=modal.gpu.H100(count=8),\n",
    "              secrets=[modal.Secret.from_name(\"HF_SECRET\")],\n",
    "              timeout=20 * 60 * 60,  # 20 hours\n",
    "              )\n",
    "def train_model():\n",
    "    # Import modules inside the function so they load from the Modal image.\n",
    "    import os\n",
    "    from huggingface_hub import login\n",
    "    from datasets import Dataset\n",
    "    login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=True)\n",
    "    print(\"Logged into HF Hub in remote training function.\")\n",
    "    \n",
    "    import json\n",
    "    # Import TRL and related modules inside the function.\n",
    "    from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "\n",
    "    # Load the datasets from the mounted directory.\n",
    "    with open(\"/mnt/data/train_dataset.json\") as f:\n",
    "        train_dataset = Dataset.from_dict(json.load(f))\n",
    "    with open(\"/mnt/data/test_dataset.json\") as f:\n",
    "        test_dataset = Dataset.from_dict(json.load(f))\n",
    "    print(\"Loaded datasets.\")\n",
    "    # Define the model's configuration.\n",
    "    model_config = ModelConfig(\n",
    "        model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        torch_dtype=\"bfloat16\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        use_peft=True,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    print(\"Build model config.\")\n",
    "    # Specify GRPO training hyperparameters.\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=\"qwen-r1-aha-moment\",\n",
    "        learning_rate=5e-7,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        max_steps=20,  # Example: Increase for real training.\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        bf16=True,\n",
    "        max_prompt_length=256,\n",
    "        max_completion_length=1024,\n",
    "        num_generations=2,\n",
    "        beta=0.001,\n",
    "    )\n",
    "\n",
    "    # Initialize the GRPO trainer.\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model_config.model_name_or_path,\n",
    "        reward_funcs=[format_reward_func, equation_reward_func],\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=get_peft_config(model_config),\n",
    "    )\n",
    "\n",
    "    print(\"Starting training on remote GPU...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "    print(\"Training complete; model saved to:\", training_args.output_dir)\n",
    "    trainer.push_to_hub(f\"LightningRodLabs/{training_args.output_dir}\")\n",
    "    print(\"Model uploaded to HuggingFace Hub:\", f\"LightningRodLabs/{training_args.output_dir}\")\n",
    "    return \"Training complete!\"\n",
    "\n",
    "with app.run():\n",
    "    result = train_model.remote()\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
